{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepGrow 2D Inference Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deepgrow is an AI Assisted Annotation tool designed for speeding up the annotation process by a user based interaction in the form of clicks. The deepgrow uses a guided predictive segmentation model, where the guidance is generated by the user in the form of positive or negative clicks. The positive clicks are guidance indicators towards the organ/region of interest, while the negative clicks are guidance signals for suggesting regions that should not be a part of the segmentation/annotation. An overview of this process is shown in the below figure:\n",
    "\n",
    "<img src=\"../../figures/image_deepgrow_scheme.png\" alt='deepgrow scheme'>\n",
    "\n",
    "based on: Sakinis et al., Interactive segmentation of medical images through\n",
    "fully convolutional neural networks. (2019) https://arxiv.org/abs/1903.08205"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import jit\n",
    "\n",
    "from monai.apps.deepgrow.transforms import (\n",
    "    AddGuidanceFromPointsd,\n",
    "    AddGuidanceSignald,\n",
    "    Fetch2DSliced,\n",
    "    ResizeGuidanced,\n",
    "    RestoreLabeld,\n",
    "    SpatialCropGuidanced,\n",
    ")\n",
    "from monai.transforms import (\n",
    "    AsChannelFirstd,\n",
    "    Spacingd,\n",
    "    LoadImaged,\n",
    "    AddChanneld,\n",
    "    NormalizeIntensityd,\n",
    "    ToTensord,\n",
    "    ToNumpyd,\n",
    "    Activationsd,\n",
    "    AsDiscreted,\n",
    "    Resized\n",
    ")\n",
    "\n",
    "max_epochs = 1\n",
    "\n",
    "\n",
    "def draw_points(guidance):\n",
    "    if guidance is None:\n",
    "        return\n",
    "    colors = ['r+', 'b+']\n",
    "    for color, points in zip(colors, guidance):\n",
    "        for p in points:\n",
    "            p1 = p[-1]\n",
    "            p2 = p[-2]\n",
    "            plt.plot(p1, p2, color, 'MarkerSize', 30)\n",
    "\n",
    "\n",
    "def show_image(image, label, guidance=None):\n",
    "    plt.figure(\"check\", (12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"image\")\n",
    "    plt.imshow(image, cmap=\"gray\")\n",
    "\n",
    "    if label is not None:\n",
    "        masked = np.ma.masked_where(label == 0, label)\n",
    "        plt.imshow(masked, 'jet', interpolation='none', alpha=0.7)\n",
    "\n",
    "    draw_points(guidance)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if label is not None:\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title(\"label\")\n",
    "        plt.imshow(label)\n",
    "        plt.colorbar()\n",
    "        # draw_points(guidance)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def print_data(data):\n",
    "    for k in data:\n",
    "        v = data[k]\n",
    "\n",
    "        d = type(v)\n",
    "        if type(v) in (int, float, bool, str, dict, tuple):\n",
    "            d = v\n",
    "        elif hasattr(v, 'shape'):\n",
    "            d = v.shape\n",
    "\n",
    "        if k in ('image_meta_dict', 'label_meta_dict'):\n",
    "            for m in data[k]:\n",
    "                print('{} Meta:: {} => {}'.format(k, m, data[k][m]))\n",
    "        else:\n",
    "            print('Data key: {} = {}'.format(k, d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing hyper-parameters and Transform compositions. Image is resampled to a 1.0x1.0 mm^2 resolution. The below snippet shows where the guidance signal is placed on the foreground (organ of interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre Processing\n",
    "roi_size = [256, 256]\n",
    "model_size = [128, 192, 192]\n",
    "pixdim = (1.0, 1.0)\n",
    "dimensions = 2\n",
    "\n",
    "data = {\n",
    "    'image': '_image.nii.gz',\n",
    "    'foreground': [[66, 180, 105]],\n",
    "    'background': []\n",
    "}\n",
    "slice_idx = original_slice_idx = data['foreground'][0][2]\n",
    "\n",
    "pre_transforms = [\n",
    "    LoadImaged(keys='image'),\n",
    "    AsChannelFirstd(keys='image'),\n",
    "    Spacingd(keys='image', pixdim=pixdim, mode='bilinear'),\n",
    "\n",
    "    AddGuidanceFromPointsd(ref_image='image', guidance='guidance', foreground='foreground', background='background',\n",
    "                           dimensions=dimensions),\n",
    "    Fetch2DSliced(keys='image', guidance='guidance'),\n",
    "    AddChanneld(keys='image'),\n",
    "\n",
    "    SpatialCropGuidanced(keys='image', guidance='guidance', spatial_size=roi_size),\n",
    "    Resized(keys='image', spatial_size=roi_size, mode='area'),\n",
    "    ResizeGuidanced(guidance='guidance', ref_image='image'),\n",
    "    NormalizeIntensityd(keys='image', subtrahend=208.0, divisor=388.0),\n",
    "    AddGuidanceSignald(image='image', guidance='guidance'),\n",
    "    ToTensord(keys='image')\n",
    "]\n",
    "\n",
    "original_image = None\n",
    "original_image_slice = None\n",
    "for t in pre_transforms:\n",
    "    tname = type(t).__name__\n",
    "\n",
    "    data = t(data)\n",
    "    image = data['image']\n",
    "    label = data.get('label')\n",
    "    guidance = data.get('guidance')\n",
    "\n",
    "    print(\"{} => image shape: {}, label shape: {}\".format(\n",
    "        tname, image.shape, label.shape if label is not None else None))\n",
    "\n",
    "    image = image if tname == 'Fetch2DSliced' else image[:, :, slice_idx] if tname in (\n",
    "        'LoadImaged') else image[slice_idx, :, :]\n",
    "    label = None\n",
    "\n",
    "    guidance = guidance if guidance else [np.roll(data['foreground'], 1).tolist(), []]\n",
    "    print('Guidance: {}'.format(guidance))\n",
    "\n",
    "    show_image(image, label, guidance)\n",
    "    if tname == 'Fetch2DSliced':\n",
    "        slice_idx = 0\n",
    "    if tname == 'LoadImaged':\n",
    "        original_image = data['image']\n",
    "    if tname == 'AddChanneld':\n",
    "        original_image_slice = data['image']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a single click, the prediction is made from the deepgrow model. Corresponding input image with the known ground truth is shown along with the predicted segmentation. They has been shown for multiple slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "model_path = '/workspace/Data/models/deepgrow_2d.ts'\n",
    "model = jit.load(model_path)\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "inputs = data['image'][None].cuda()\n",
    "with torch.no_grad():\n",
    "    outputs = model(inputs)\n",
    "outputs = outputs[0]\n",
    "data['pred'] = outputs\n",
    "\n",
    "post_transforms = [\n",
    "    Activationsd(keys='pred', sigmoid=True),\n",
    "    AsDiscreted(keys='pred', threshold_values=True, logit_thresh=0.5),\n",
    "    ToNumpyd(keys='pred'),\n",
    "    RestoreLabeld(keys='pred', ref_image='image', mode='nearest'),\n",
    "]\n",
    "\n",
    "for t in post_transforms:\n",
    "    tname = type(t).__name__\n",
    "\n",
    "    data = t(data)\n",
    "    image = original_image if tname == 'RestoreLabeld' else data['image']\n",
    "    label = data['pred']\n",
    "    print(\"{} => image shape: {}, pred shape: {}\".format(tname, image.shape, label.shape))\n",
    "\n",
    "    if tname in 'RestoreLabeld':\n",
    "        image = image[:, :, original_slice_idx]\n",
    "        label = label[0, :, :].detach().cpu().numpy() if torch.is_tensor(label) else label[original_slice_idx]\n",
    "        print(\"PLOT:: {} => image shape: {}, pred shape: {}; min: {}, max: {}, sum: {}\".format(\n",
    "            tname, image.shape, label.shape, np.min(label), np.max(label), np.sum(label)))\n",
    "        show_image(image, label)\n",
    "    else:\n",
    "        image = image[0, :, :].detach().cpu().numpy() if torch.is_tensor(image) else image[0]\n",
    "        label = label[0, :, :].detach().cpu().numpy() if torch.is_tensor(label) else label[0]\n",
    "        print(\"PLOT:: {} => image shape: {}, pred shape: {}; min: {}, max: {}, sum: {}\".format(\n",
    "            tname, image.shape, label.shape, np.min(label), np.max(label), np.sum(label)))\n",
    "        show_image(image, label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
